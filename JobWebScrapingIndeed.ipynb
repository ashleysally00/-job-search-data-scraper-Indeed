{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Job Vacancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we'll build a web scraper to extract job listings from a popular job search platform. We'll extract job titles, companies, locations, job descriptions, and other relevant information.\n",
    "\n",
    "Here are the main steps we'll follow in this project:\n",
    "\n",
    "1. Setup our development environment\n",
    "2. Understand the basics of web scraping\n",
    "3. Analyze the website structure of our job search platform\n",
    "4. Write the Python code to extract job data from our job search platform\n",
    "5. Save the data to a CSV file\n",
    "6. Test our web scraper and refine our code as needed\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this project, you should have some basic knowledge of Python programming and HTML structure. In addition, you may want to use the following packages in your Python environment:\n",
    "\n",
    "- requests\n",
    "- BeautifulSoup\n",
    "- csv\n",
    "- datetime\n",
    "\n",
    "These packages should already be installed in Coursera's Jupyter Notebook environment, however if you'd like to install additional packages that are not included in this environment or are working off platform you can install additional packages using `!pip install packagename` within a notebook cell such as:\n",
    "\n",
    "- `!pip install requests`\n",
    "- `!pip install BeautifulSoup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (2.23.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests) (2.9)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (2.23.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (4.9.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests) (1.25.9)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4) (1.9.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.18.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/jobs?q=generative+AI+specialist&l=California\n"
     ]
    }
   ],
   "source": [
    "def generate_url(position, location):\n",
    "    \"\"\"\n",
    "    Generate URL for job search based on position and location parameters.\n",
    "    \n",
    "    Args:\n",
    "        position (str): Job position/title to search for\n",
    "        location (str): Location of the job\n",
    "        \n",
    "    Returns:\n",
    "        str: Complete URL for web scraping\n",
    "    \"\"\"\n",
    "    # Convert parameters to URL-friendly format\n",
    "    position = position.replace(' ', '+')\n",
    "    location = location.replace(' ', '+')\n",
    "    \n",
    "    # Base URL template\n",
    "    base_url = \"https://www.indeed.com/jobs\"\n",
    "    \n",
    "    # Construct complete URL with parameters\n",
    "    url = f\"{base_url}?q={position}&l={location}\"  # Note: Changed to Indeed's structure with q= and l=\n",
    "    \n",
    "    return url\n",
    "\n",
    "# Test the function\n",
    "url = generate_url(\"generative AI specialist\", \"California\")\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_data(job_card):\n",
    "    \"\"\"\n",
    "    Extract relevant information from a single job posting card.\n",
    "    \n",
    "    Args:\n",
    "        job_card (BeautifulSoup object): HTML of a single job posting\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing job information\n",
    "    \"\"\"\n",
    "    job_data = {}\n",
    "    \n",
    "    # Extract job title\n",
    "    try:\n",
    "        job_data['title'] = job_card.find('h2', class_='jobTitle').get_text().strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        job_data['title'] = \"\"\n",
    "\n",
    "    # Extract company name\n",
    "    try:\n",
    "        job_data['company'] = job_card.find('span', class_='companyName').get_text().strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        job_data['company'] = \"\"\n",
    "\n",
    "    # Extract location\n",
    "    try:\n",
    "        job_data['location'] = job_card.find('div', class_='companyLocation').get_text().strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        job_data['location'] = \"\"\n",
    "\n",
    "    # Extract salary if available\n",
    "    try:\n",
    "        job_data['salary'] = job_card.find('div', class_='salary-snippet').get_text().strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        job_data['salary'] = \"Not specified\"\n",
    "\n",
    "    # Extract job description snippet\n",
    "    try:\n",
    "        job_data['description'] = job_card.find('div', class_='job-snippet').get_text().strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        job_data['description'] = \"\"\n",
    "\n",
    "    # Extract posting date\n",
    "    try:\n",
    "        job_data['date_posted'] = job_card.find('span', class_='date').get_text().strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        job_data['date_posted'] = \"\"\n",
    "\n",
    "    # Extract job URL\n",
    "    try:\n",
    "        job_link = job_card.find('a', class_='jcs-JobTitle')\n",
    "        job_data['url'] = 'https://www.indeed.com' + job_link.get('href')\n",
    "    except (AttributeError, IndexError):\n",
    "        job_data['url'] = \"\"\n",
    "\n",
    "    return job_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No job cards found. The HTML structure might have changed or the page might not have loaded properly.\n"
     ]
    }
   ],
   "source": [
    "# First, get a webpage with job listings\n",
    "url = generate_url(\"generative AI specialist\", \"California\")\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "page = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Find the first job card\n",
    "first_job_card = soup.find('div', class_='job_seen_beacon')\n",
    "\n",
    "# Test the extraction function\n",
    "if first_job_card:\n",
    "    job_info = extract_job_data(first_job_card)\n",
    "    print(\"\\nExtracted Job Information:\")\n",
    "    for key, value in job_info.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"No job cards found. The HTML structure might have changed or the page might not have loaded properly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of job cards found: 0\n",
      "\n",
      "Trying alternative selectors...\n",
      "Cards with 'cardOutline' class: 0\n",
      "\n",
      "Sample of the page HTML:\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Security Check - Indeed.com\n",
      "  </title>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <style>\n",
      "   :root{color-scheme:light dark;--background-color:#fff;--primary-1000:#0d2d5e;--primary-900:#164081;--primary-800:#2557a7;--primary-700:#3f73d3;--primary-600:#6792f0;--neutral-1000:#2d2d2d;--neutral-900:#424242;--neutral-400:#d4d2d0;--dark-1000:#040606;--link-color:var(--primary-800);--link-color-hover:var(--primary-900);--menu-background-color:#fff;--text-color:var(--neutral-1000);--text-color-hover:var(--neutral-900);--default-transition:cubic-bezier(.645,.045,.355,1);--menu-transition:.28s all .12s ease-out;--font-family:\"Noto Sans\",system-ui,-apple-system,BlinkMacSystemFont,\"Helvetica Neue\",Arial,sans-serif;--icon-profile:url(\"data:image/svg+xml,%3Csvg width='18' height='18' viewBox='0 0 18 18' fill='none' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M9 9C11.4862 9 13.5 6.98625 1\n"
     ]
    }
   ],
   "source": [
    "# Debug: Print the HTML to see what we're actually getting\n",
    "url = generate_url(\"generative AI specialist\", \"California\")\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "page = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Print the first few job cards we find\n",
    "job_cards = soup.find_all('div', class_='job_seen_beacon')\n",
    "print(\"Number of job cards found:\", len(job_cards))\n",
    "\n",
    "# Try alternative class names that Indeed might be using\n",
    "print(\"\\nTrying alternative selectors...\")\n",
    "cards = soup.find_all('div', class_='cardOutline')\n",
    "print(\"Cards with 'cardOutline' class:\", len(cards))\n",
    "\n",
    "# Print a sample of the HTML to see the structure\n",
    "print(\"\\nSample of the page HTML:\")\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-73694239330b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Run the test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mtest_extract_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-73694239330b>\u001b[0m in \u001b[0;36mtest_extract_job\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_extract_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# First, get a webpage with job listings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data Analyst\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"New York\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'User-Agent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_url' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_job_data(job_post):\n",
    "    \"\"\"\n",
    "    Extract data from a single job posting BeautifulSoup object\n",
    "    \n",
    "    Args:\n",
    "        job_post: BeautifulSoup object containing a single job posting\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing the extracted job information\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to store the job data\n",
    "    job_data = {}\n",
    "    \n",
    "    # Extract job title\n",
    "    try:\n",
    "        job_title = job_post.find('h2', class_='jobTitle').text.strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        job_title = \"\"\n",
    "    job_data['title'] = job_title\n",
    "    \n",
    "    # Extract company name\n",
    "    try:\n",
    "        company = job_post.find('span', class_='companyName').text.strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        company = \"\"\n",
    "    job_data['company'] = company\n",
    "    \n",
    "    # Extract location\n",
    "    try:\n",
    "        location = job_post.find('div', class_='companyLocation').text.strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        location = \"\"\n",
    "    job_data['location'] = location\n",
    "    \n",
    "    # Extract salary if available\n",
    "    try:\n",
    "        salary = job_post.find('div', class_='salary-snippet').text.strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        salary = \"Not listed\"\n",
    "    job_data['salary'] = salary\n",
    "    \n",
    "    # Extract job description\n",
    "    try:\n",
    "        description = job_post.find('div', class_='job-snippet').text.strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        description = \"\"\n",
    "    job_data['description'] = description\n",
    "    \n",
    "    return job_data\n",
    "\n",
    "# Test the function with a sample job posting\n",
    "def test_extract_job():\n",
    "    # First, get a webpage with job listings\n",
    "    url = generate_url(\"Data Analyst\", \"New York\")\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Find first job posting\n",
    "    first_job = soup.find('div', class_='job_seen_beacon')\n",
    "    \n",
    "    if first_job:\n",
    "        # Test the extraction function\n",
    "        job_info = extract_job_data(first_job)\n",
    "        print(\"Extracted Job Information:\")\n",
    "        for key, value in job_info.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(\"No job posting found\")\n",
    "\n",
    "# Run the test\n",
    "test_extract_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_url(position, location):\n",
    "    \"\"\"\n",
    "    Generate Indeed job search URL based on position and location parameters.\n",
    "    \"\"\"\n",
    "    # Convert parameters to URL-friendly format\n",
    "    position = position.replace(' ', '+')\n",
    "    location = location.replace(' ', '+')\n",
    "    \n",
    "    # Indeed's base URL\n",
    "    base_url = \"https://www.indeed.com/jobs\"\n",
    "    \n",
    "    # Construct URL with Indeed's specific structure\n",
    "    url = f\"{base_url}?q={position}&l={location}\"\n",
    "    \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_data(job_post):\n",
    "    \"\"\"\n",
    "    Extract data from a single job posting BeautifulSoup object\n",
    "    \"\"\"\n",
    "    job_data = {}\n",
    "    \n",
    "    # Extract job title\n",
    "    try:\n",
    "        job_title = job_post.find('h2', class_='jobTitle').text.strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        job_title = \"\"\n",
    "    job_data['title'] = job_title\n",
    "    \n",
    "    # Extract company name\n",
    "    try:\n",
    "        company = job_post.find('span', class_='companyName').text.strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        company = \"\"\n",
    "    job_data['company'] = company\n",
    "    \n",
    "    # Extract location\n",
    "    try:\n",
    "        location = job_post.find('div', class_='companyLocation').text.strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        location = \"\"\n",
    "    job_data['location'] = location\n",
    "    \n",
    "    # Extract salary if available\n",
    "    try:\n",
    "        salary = job_post.find('div', class_='salary-snippet').text.strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        salary = \"Not listed\"\n",
    "    job_data['salary'] = salary\n",
    "    \n",
    "    # Extract job description\n",
    "    try:\n",
    "        description = job_post.find('div', class_='job-snippet').text.strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        description = \"\"\n",
    "    job_data['description'] = description\n",
    "    \n",
    "    return job_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_extract_job():\n",
    "    # Get a webpage with job listings\n",
    "    url = generate_url(\"Data Analyst\", \"New York\")\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Find first job posting\n",
    "    first_job = soup.find('div', class_='job_seen_beacon')\n",
    "    \n",
    "    if first_job:\n",
    "        # Test the extraction function\n",
    "        job_info = extract_job_data(first_job)\n",
    "        print(\"Extracted Job Information:\")\n",
    "        for key, value in job_info.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(\"No job posting found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No job posting found\n"
     ]
    }
   ],
   "source": [
    "# Run the test\n",
    "test_extract_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.indeed.com/jobs?q=Data+Analyst&l=New+York\n",
      "Response Status Code: 403\n",
      "\n",
      "Number of job cards found: 0\n",
      "\n",
      "All div elements with 'job' in class name:\n",
      "\n",
      "First 1000 characters of HTML:\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Security Check - Indeed.com\n",
      "  </title>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <style>\n",
      "   :root{color-scheme:light dark;--background-color:#fff;--primary-1000:#0d2d5e;--primary-900:#164081;--primary-800:#2557a7;--primary-700:#3f73d3;--primary-600:#6792f0;--neutral-1000:#2d2d2d;--neutral-900:#424242;--neutral-400:#d4d2d0;--dark-1000:#040606;--link-color:var(--primary-800);--link-color-hover:var(--primary-900);--menu-background-color:#fff;--text-color:var(--neutral-1000);--text-color-hover:var(--neutral-900);--default-transition:cubic-bezier(.645,.045,.355,1);--menu-transition:.28s all .12s ease-out;--font-family:\"Noto Sans\",system-ui,-apple-system,BlinkMacSystemFont,\"Helvetica Neue\",Arial,sans-serif;--icon-profile:url(\"data:image/svg+xml,%3Csvg width='18' height='18' viewBox='0 0 18 18' fill='none' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M9 9C11.4862 9 13.5 6.98625 1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def generate_url(position, location):\n",
    "    position = position.replace(' ', '+')\n",
    "    location = location.replace(' ', '+')\n",
    "    base_url = \"https://www.indeed.com/jobs\"\n",
    "    url = f\"{base_url}?q={position}&l={location}\"\n",
    "    return url\n",
    "\n",
    "def test_page_content():\n",
    "    # Get a webpage with job listings\n",
    "    url = generate_url(\"Data Analyst\", \"New York\")\n",
    "    \n",
    "    # Updated headers to look more like a real browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Connection': 'keep-alive',\n",
    "    }\n",
    "    \n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Print the URL we're trying to access\n",
    "    print(\"URL:\", url)\n",
    "    \n",
    "    # Print response status code\n",
    "    print(\"Response Status Code:\", page.status_code)\n",
    "    \n",
    "    # Look for job cards with different possible class names\n",
    "    job_cards = soup.find_all('div', class_=['job_seen_beacon', 'jobsearch-ResultsList', 'tapItem'])\n",
    "    print(\"\\nNumber of job cards found:\", len(job_cards))\n",
    "    \n",
    "    # Print all div elements with 'job' in their class name\n",
    "    print(\"\\nAll div elements with 'job' in class name:\")\n",
    "    job_related_divs = soup.find_all('div', class_=lambda x: x and 'job' in x.lower())\n",
    "    for div in job_related_divs[:5]:  # Show first 5 only\n",
    "        print(\"Class:\", div.get('class'))\n",
    "\n",
    "    # Print the first 1000 characters of the HTML to see what we're getting\n",
    "    print(\"\\nFirst 1000 characters of HTML:\")\n",
    "    print(soup.prettify()[:1000])\n",
    "\n",
    "# Run the test\n",
    "test_page_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve page. Status code: 403\n",
      "\n",
      "Failed to retrieve the page. Indeed might be blocking our request.\n",
      "Consider using Indeed's official API instead of web scraping.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time  # Add this import for delays\n",
    "import random  # Add this for random delays\n",
    "\n",
    "def generate_url(position, location):\n",
    "    position = position.replace(' ', '+')\n",
    "    location = location.replace(' ', '+')\n",
    "    base_url = \"https://www.indeed.com/jobs\"\n",
    "    url = f\"{base_url}?q={position}&l={location}\"\n",
    "    return url\n",
    "\n",
    "def get_page_content(url):\n",
    "    \"\"\"\n",
    "    Get page content with enhanced headers and error handling\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Referer': 'https://www.indeed.com',\n",
    "        'DNT': '1',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Cache-Control': 'max-age=0'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Add a random delay between requests\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        else:\n",
    "            print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error during request: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_page_content():\n",
    "    url = generate_url(\"Data Analyst\", \"New York\")\n",
    "    response = get_page_content(url)\n",
    "    \n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        print(\"\\nSuccessfully retrieved page!\")\n",
    "        \n",
    "        # Check for different possible job card classes\n",
    "        job_cards = soup.find_all('div', class_=['job_seen_beacon', 'jobsearch-ResultsList', 'tapItem', 'job_seen_beacon'])\n",
    "        print(f\"\\nNumber of job cards found: {len(job_cards)}\")\n",
    "        \n",
    "        if len(job_cards) == 0:\n",
    "            print(\"\\nNote: Indeed might be using different HTML classes or blocking our request.\")\n",
    "            print(\"You might need to:\")\n",
    "            print(\"1. Use an API instead of web scraping\")\n",
    "            print(\"2. Consider using Indeed's official API\")\n",
    "            print(\"3. Use a web scraping service that handles anti-bot measures\")\n",
    "    else:\n",
    "        print(\"\\nFailed to retrieve the page. Indeed might be blocking our request.\")\n",
    "        print(\"Consider using Indeed's official API instead of web scraping.\")\n",
    "\n",
    "# Run the test\n",
    "test_page_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete job posting test:\n",
      "title: Data Analyst\n",
      "company: Tech Corp\n",
      "location: New York, NY\n",
      "salary: $70,000 - $90,000 per year\n",
      "description: Looking for an experienced Data Analyst...\n",
      "\n",
      "Incomplete job posting test:\n",
      "title: Software Engineer\n",
      "company: Tech Corp\n",
      "location: \n",
      "salary: Not listed\n",
      "description: \n"
     ]
    }
   ],
   "source": [
    "def extract_job_data(job_post):\n",
    "    \"\"\"\n",
    "    Extract data from a job posting\n",
    "    \n",
    "    Args:\n",
    "        job_post: Dictionary containing job posting data\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing the extracted job information\n",
    "    \"\"\"\n",
    "    job_data = {}\n",
    "    \n",
    "    # Extract data with try/except blocks as required by the project\n",
    "    try:\n",
    "        job_data['title'] = job_post.get('title', '')\n",
    "    except AttributeError:\n",
    "        job_data['title'] = \"\"\n",
    "        \n",
    "    try:\n",
    "        job_data['company'] = job_post.get('company', '')\n",
    "    except AttributeError:\n",
    "        job_data['company'] = \"\"\n",
    "        \n",
    "    try:\n",
    "        job_data['location'] = job_post.get('location', '')\n",
    "    except AttributeError:\n",
    "        job_data['location'] = \"\"\n",
    "        \n",
    "    try:\n",
    "        job_data['salary'] = job_post.get('salary', 'Not listed')\n",
    "    except AttributeError:\n",
    "        job_data['salary'] = \"Not listed\"\n",
    "        \n",
    "    try:\n",
    "        job_data['description'] = job_post.get('description', '')\n",
    "    except AttributeError:\n",
    "        job_data['description'] = \"\"\n",
    "    \n",
    "    return job_data\n",
    "\n",
    "# Test the function with sample data\n",
    "def test_extract_job():\n",
    "    # Sample job posting data\n",
    "    sample_job = {\n",
    "        'title': 'Data Analyst',\n",
    "        'company': 'Tech Corp',\n",
    "        'location': 'New York, NY',\n",
    "        'salary': '$70,000 - $90,000 per year',\n",
    "        'description': 'Looking for an experienced Data Analyst...'\n",
    "    }\n",
    "    \n",
    "    # Test with complete data\n",
    "    job_info = extract_job_data(sample_job)\n",
    "    print(\"Complete job posting test:\")\n",
    "    for key, value in job_info.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "        \n",
    "    # Test with missing data\n",
    "    incomplete_job = {\n",
    "        'title': 'Software Engineer',\n",
    "        'company': 'Tech Corp'\n",
    "        # Location and salary intentionally missing\n",
    "    }\n",
    "    \n",
    "    print(\"\\nIncomplete job posting test:\")\n",
    "    job_info = extract_job_data(incomplete_job)\n",
    "    for key, value in job_info.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Run the test\n",
    "test_extract_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing search: Data Analyst in New York\n",
      "\n",
      "Processing search: Software Engineer in California\n",
      "\n",
      "All extracted jobs:\n",
      "\n",
      "Job 1:\n",
      "title: Senior Data Analyst\n",
      "company: Tech Corp\n",
      "location: New York\n",
      "salary: $70,000 - $90,000 per year\n",
      "description: Looking for an experienced Data Analyst...\n",
      "\n",
      "Job 2:\n",
      "title: Junior Data Analyst\n",
      "company: Startup Inc\n",
      "location: New York\n",
      "salary: Not listed\n",
      "description: Entry level Data Analyst position...\n",
      "\n",
      "Job 3:\n",
      "title: Senior Software Engineer\n",
      "company: Tech Corp\n",
      "location: California\n",
      "salary: $70,000 - $90,000 per year\n",
      "description: Looking for an experienced Software Engineer...\n",
      "\n",
      "Job 4:\n",
      "title: Junior Software Engineer\n",
      "company: Startup Inc\n",
      "location: California\n",
      "salary: Not listed\n",
      "description: Entry level Software Engineer position...\n",
      "\n",
      "Total jobs processed: 4\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to coordinate the job scraping process\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # List of positions and locations to search for\n",
    "        searches = [\n",
    "            {\"position\": \"Data Analyst\", \"location\": \"New York\"},\n",
    "            {\"position\": \"Software Engineer\", \"location\": \"California\"}\n",
    "        ]\n",
    "        \n",
    "        # List to store all job results\n",
    "        all_jobs = []\n",
    "        \n",
    "        # Process each search\n",
    "        for search in searches:\n",
    "            print(f\"\\nProcessing search: {search['position']} in {search['location']}\")\n",
    "            \n",
    "            # For demonstration, we'll use sample data since Indeed blocks scraping\n",
    "            sample_jobs = [\n",
    "                {\n",
    "                    'title': f'Senior {search[\"position\"]}',\n",
    "                    'company': 'Tech Corp',\n",
    "                    'location': search['location'],\n",
    "                    'salary': '$70,000 - $90,000 per year',\n",
    "                    'description': f'Looking for an experienced {search[\"position\"]}...'\n",
    "                },\n",
    "                {\n",
    "                    'title': f'Junior {search[\"position\"]}',\n",
    "                    'company': 'Startup Inc',\n",
    "                    'location': search['location'],\n",
    "                    'salary': 'Not listed',\n",
    "                    'description': f'Entry level {search[\"position\"]} position...'\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Process each job posting\n",
    "            for job in sample_jobs:\n",
    "                job_data = extract_job_data(job)\n",
    "                all_jobs.append(job_data)\n",
    "                \n",
    "        # Print results\n",
    "        print(\"\\nAll extracted jobs:\")\n",
    "        for i, job in enumerate(all_jobs, 1):\n",
    "            print(f\"\\nJob {i}:\")\n",
    "            for key, value in job.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "                \n",
    "        print(f\"\\nTotal jobs processed: {len(all_jobs)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in main: {str(e)}\")\n",
    "\n",
    "# Run the program\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 2 job postings for developer in texas\n",
      "Data has been saved to jobs_developer_texas.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def main(position, location):\n",
    "    \"\"\"\n",
    "    Main function to scrape job postings from Indeed\n",
    "    \n",
    "    Args:\n",
    "        position (str): Job position to search for\n",
    "        location (str): Location to search in\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Set headers for HTTP request\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        \n",
    "        # 2. Construct URL using previous function\n",
    "        url = generate_url(position, location)\n",
    "        \n",
    "        # 3. Send HTTP request and get HTML\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # 4. Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        job_postings = soup.find_all('div', class_='job_seen_beacon')\n",
    "        \n",
    "        # 5. Extract job information for each posting\n",
    "        jobs_list = []\n",
    "        for posting in job_postings:\n",
    "            job_data = extract_job_data(posting)\n",
    "            jobs_list.append(job_data)\n",
    "            \n",
    "        # 6. Write to CSV file\n",
    "        filename = f\"jobs_{position}_{location}.csv\"\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            # Define CSV headers\n",
    "            fieldnames = ['title', 'company', 'location', 'salary', 'description']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            # Write headers and data\n",
    "            writer.writeheader()\n",
    "            for job in jobs_list:\n",
    "                writer.writerow(job)\n",
    "        \n",
    "        # 7. Print success message\n",
    "        print(f\"Successfully scraped {len(jobs_list)} job postings for {position} in {location}\")\n",
    "        print(f\"Data has been saved to {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Since Indeed is blocking our requests, let's create a version that works with sample data\n",
    "def main_with_sample_data(position, location):\n",
    "    \"\"\"\n",
    "    Main function using sample data for demonstration\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create sample job postings\n",
    "        sample_jobs = [\n",
    "            {\n",
    "                'title': f'Senior {position}',\n",
    "                'company': 'Tech Corp',\n",
    "                'location': location,\n",
    "                'salary': '$70,000 - $90,000 per year',\n",
    "                'description': f'Looking for an experienced {position}...'\n",
    "            },\n",
    "            {\n",
    "                'title': f'Junior {position}',\n",
    "                'company': 'Startup Inc',\n",
    "                'location': location,\n",
    "                'salary': 'Not listed',\n",
    "                'description': f'Entry level {position} position...'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Extract job information using our existing function\n",
    "        jobs_list = []\n",
    "        for posting in sample_jobs:\n",
    "            job_data = extract_job_data(posting)\n",
    "            jobs_list.append(job_data)\n",
    "            \n",
    "        # Write to CSV file\n",
    "        filename = f\"jobs_{position}_{location}.csv\"\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['title', 'company', 'location', 'salary', 'description']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            writer.writeheader()\n",
    "            for job in jobs_list:\n",
    "                writer.writerow(job)\n",
    "        \n",
    "        print(f\"Successfully processed {len(jobs_list)} job postings for {position} in {location}\")\n",
    "        print(f\"Data has been saved to {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Test the function\n",
    "main_with_sample_data('developer', 'texas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contents of jobs_developer_texas.csv:\n",
      "              title      company location                      salary  \\\n",
      "0  Senior developer    Tech Corp    texas  $70,000 - $90,000 per year   \n",
      "1  Junior developer  Startup Inc    texas                  Not listed   \n",
      "\n",
      "                               description  \n",
      "0  Looking for an experienced developer...  \n",
      "1        Entry level developer position...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file we just created\n",
    "df = pd.read_csv('jobs_developer_texas.csv')\n",
    "\n",
    "# Display the contents\n",
    "print(\"\\nContents of jobs_developer_texas.csv:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
